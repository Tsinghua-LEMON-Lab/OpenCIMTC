from ..utils.python import PythonCode
from typing import Callable
from ...irtool.core import BaseIR
from ...irtool.tools import flatten_layers  # noqa
from ...hw_paras_def.macro import *

class InferenceCodeGen(PythonCode):
    
    def __init__(self, ir, module_name='Inference', inputs=None, outputs=None, simulation=True, is4search=False, is4label=False, is4program=False):
        super().__init__()
        self.ir = ir
        self.module_name = module_name
        self.inputs = inputs
        self.outputs = outputs
        self.simulation = simulation
        self.is4search = is4search
        self.is4label = is4label
        self.is4program = is4program
        
    def gen_notes(self):
        yield '''
# **************************************** #
# This file is automatically generated !!! #
#        Please do not modify it !!!      #
# **************************************** #
'''
    
    def gen_imports(self):
        if self.simulation:
            yield f'from OpenCIMTC.simulator.torch_utils import *'
        else:
            yield f'from OpenCIMTC.chip.torch_utils import *'
        
    def gen_layers(self):
        
        yield from self.gen_notes()
        yield from self.gen_imports()
        
        yield 
        yield

        # sort layers
        self.ir.layers = dict(self.ir.iter_layers(deep=False, sorted=True)) 
        
        assert isinstance(self.ir, BaseIR), f'invalid IR type={type(self.ir)}'
        layers = self.ir.flatten_layers()
        
        # init inputs
        inp, oup = self.ir.get_io_layers(layers)
        
        # record all nodes
        self.all_node = {}
        
        # record all data type
        self.layer_out_data_type = self.get_data_type(layers)
        
        # 
        self.next_layers = self.get_next_layer(layers)
        self.pre_layers = self.get_pre_layer(layers)
        
        # remove unused layers
        remove_layers = []
        if self.inputs != None:
            assert self.inputs in self.pre_layers.keys(), f'{self.inputs}'
            self.get_all_pre_layers(self.inputs, remove_layers)
            remove_layers.append(self.inputs)
            
        if self.outputs != None:
            assert self.outputs in self.next_layers.keys(), f'{self.outputs}'
            self.get_all_next_layers(self.outputs, remove_layers)
        
        if remove_layers != None:
            # replace the input and output information in the original graph
            if self.inputs != None:
                next_layers = self.next_layers[self.inputs]
                for n in next_layers:
                    for j in layers[n].inputs:
                        if j.ref == self.inputs:
                            j.ref = 'graph_input:0'
                layers['graph_input'].inputs = layers[self.inputs].inputs
            if self.outputs != None:
                layers['graph_output'].inputs[0].ref = self.outputs
                layers['graph_output'].inputs[0].width = layers[self.outputs].outputs[0].width
                layers['graph_output'].inputs[0].height = layers[self.outputs].outputs[0].height
                layers['graph_output'].inputs[0].channel = layers[self.outputs].outputs[0].channel
            for n in remove_layers:
                layers.pop(n)
        
        for name, layer in layers.items():
            
            # label each layer
            if layer.type == 'input':
                c_ = 0
                for in_ in layer.inputs:
                    self.all_node[f'graph_input:{c_}'] = f'x_graph_input_{c_}'
                    c_ += 1
            elif layer.type == 'op':
                if len(layer.outputs) > 1:
                    c_ = 0
                    for in_ in layer.outputs:
                        self.all_node[f'{name}:{c_}'] = f'x_{name}_{c_}'
                        c_ += 1
                else:
                    self.all_node[f'{name}'] = f'x_{name}'
        
        # get all tunable parameters
        self.it_time_all = []
        self.weight_copy_number_all = []
        self.input_expansion_mode_all = []
        self.layer_index = {}
        
        self.output_layers = []
        c = 0
        for name, layer in layers.items():
            if layer.type == 'op' and layer.op.op_id in ['conv2d', 'matmul', 'fused_conv2d', 'fused_fc', 'conv_transpose2d']:
                if layer.macro_calc_info != None:
                    self.it_time_all.append(layer.macro_calc_info.it_time)
                    if layer.macro_calc_info.shift_expansion_mode == 'bit_shift':
                        self.input_expansion_mode_all.append(1)
                    elif layer.macro_calc_info.shift_expansion_mode == 'bit_pulse':
                        self.input_expansion_mode_all.append(0)
                    else:
                        raise ValueError(f'Not support input mode: {layer.macro_calc_info.shift_expansion_mode} !!!')
                    self.weight_copy_number_all.append(layer.macro_mapping_info.row_repeat_num)
                    self.layer_index[name] = c
                    c += 1
            if layer.type == 'output':
                for o in layer.inputs:
                    self.output_layers.append(o.ref)
        
        
        # gen forward function   
        for name, layer in layers.items():
            
            # input function
            if layer.type == 'input':
                c_ = 0
                with self.indent():
                    s_ = ''
                    for in_ in layer.inputs:
                        t = self.all_node[f"graph_input:{c_}"]
                        s_ += f'{t}'
                        if c_ != len(layer.inputs):
                            s_ += f', '
                        c_ += 1
                 # generate function name
                if self.is4search:
                    if not self.simulation:
                        # on chip search
                        yield f'''def {self.module_name}({s_} weights, test_labels, device='cpu'):'''
                        with self.indent():
                            yield "Optim_it_time_all = {}"
                    else:
                        yield f'''def {self.module_name}({s_} weights, PE_weight_noise=0.0, it_time={self.it_time_all}, '''
                        with self.indent(7):
                            yield f"input_expansion_mode={self.input_expansion_mode_all}, "
                            yield f"weight_copy_num={self.weight_copy_number_all}, device='cpu'):"
                elif self.simulation:
                    yield f'''def {self.module_name}({s_} weights, PE_weight_noise=0.0, device='cpu'): '''
                else:
                    if self.is4program:
                        yield f'''def {self.module_name}({s_} weights, device='cpu', program=True):'''
                    else:
                        yield f'''def {self.module_name}({s_} weights, device='cpu'):'''
                    
                # gen constant
                constant = self.gen_constants(layers)
                with self.indent():
                    yield f'# Constant Values'
                    for cs in constant:
                        yield cs
                    
            # mediate function
            if layer.type == 'op':
                with self.indent():
                    # yield f'# Op Layers: {name}'
                    s_ = self.gen_layer(layer.op.op_id, name, layer)
                    if '\n' in s_:
                        s_1 = s_.split('\n')
                        c = 0
                        for s_2 in s_1:
                            if c != 0:
                                with self.indent(7):
                                    yield s_2
                            else:
                                yield s_2
                            c += 1
                    else:
                        yield s_
                    if self.is4search and not self.simulation:
                        if layer.op.op_id in ['conv2d', 'matmul']:
                            yield f"Optim_it_time_all['{name}'] = optim_it_{name}"

            # output function
            if layer.type == 'output':
                with self.indent():
                    s_ = ''
                    if self.is4label:
                        yield "label_dict = {}"
                    for in_ in layer.inputs:
                        # get the output name and its value
                        if self.is4label:
                            pre_layer = self.pre_layers[in_.ref][0]
                            yield f"label_dict['{self.all_node[in_.ref]}_input'] = {self.all_node[pre_layer]}"
                            yield f"label_dict['{self.all_node[in_.ref]}_output'] = {self.all_node[in_.ref]}"
                        else:
                            s_ += f'{self.all_node[in_.ref]}, '
                    if self.is4search and not self.simulation:
                        s_ += 'Optim_it_time_all'
                    if self.is4label:
                        s_ = f'label_dict, '
                    yield f'return {s_}'
                    
    def get_all_pre_layers(self, layer_name, all_pre_layers):
        if layer_name in self.pre_layers.keys():
            pre_layers = self.pre_layers[layer_name]
            for n in pre_layers:
                if 'graph_input' not in n:
                    if n not in all_pre_layers:
                        all_pre_layers.append(n)
                    self.get_all_pre_layers(n, all_pre_layers)
    
    def get_all_next_layers(self, layer_name, all_next_layers):
        if layer_name in self.next_layers.keys():
            next_layers = self.next_layers[layer_name]
            for n in next_layers:
                if 'graph_output' not in n:
                    if n not in all_next_layers:
                        all_next_layers.append(n)
                    self.get_all_next_layers(n, all_next_layers)
    
    def gen_constants(self, layers):
        constant_str = []
        for name, layer in layers.items():
            if layer.type == 'op':
                if layer.op.op_id in ['constant']:
                    value = layer.value
                    constant_str.append(f'x_{name} = torch.tensor({value}).to(device)')
                elif layer.op.op_id in ['batch_norm2d']:
                    weight = layer.op.scale
                    bias = layer.op.bias
                    mean = layer.op.input_mean
                    var = layer.op.input_var
                    # 
                    index_ = name.split('_')[1]
                    
                    constant_str.append(f'x_BN_{index_}_weights = torch.tensor({weight}).to(device)')
                    constant_str.append(f'x_BN_{index_}_bias = torch.tensor({bias}).to(device)')
                    constant_str.append(f'x_BN_{index_}_mean = torch.tensor({mean}).to(device)')
                    constant_str.append(f'x_BN_{index_}_var = torch.tensor({var}).to(device)')
                    
        return constant_str
    
    def get_data_type(self, layers):
        layer_data_type = {}
        for name, layer in layers.items():
            if layer.type == 'op' and layer.op.op_id in ['conv2d', 'matmul']:
                layer_data_type[name] = layer.macro_calc_info.activation_bits
        return layer_data_type
    
    def get_pre_layer(self, layers):
        prefix_layer = {}
        for name, layer in layers.items():
            if layer.type not in ['input']:
                prefix_layer[name] =  []
                for i in layer.inputs:
                    if 'graph_input' not in i.ref:
                        ref = i.ref
                        if ':' in ref:
                            ref = ref.split(':')[0]
                        prefix_layer[name].append(ref)
                    else:        
                        prefix_layer[name].append(i.ref)
        
        return prefix_layer
    
    def get_next_layer(self, layers):
        next_layer = {}
        pre_layer = self.get_pre_layer(layers)    
        for k,v in pre_layer.items():
            for name in v:
                if name not in next_layer.keys():
                    next_layer[name] = []
                next_layer[name].append(k)
                
        return next_layer
        
    def gen_op(self, op_id, *args, **kwargs):
        fn = getattr(self, f'fn_gen_{op_id}', None)
        assert isinstance(fn, Callable), f'fn_gen_{op_id} is not a function'
        return fn(*args, **kwargs)

    def gen_layer(self, op_id, *args, **kwargs):
        return self.gen_op(op_id, *args, **kwargs)
    
    # conv
    
    def fn_gen_conv2d(self, layer_name, layer_info):
        
        # attr
        stride = layer_info.op.stride
        padding = layer_info.op.padding
        kernel = layer_info.op.kernel
        # pre layers
        pre_layers = layer_info.inputs[0].ref
        # check runtime is simulation or chip

        input_quant_scale = layer_info.macro_calc_info.input_quant_scale
        output_dequant_scale = layer_info.macro_calc_info.assigned_output_quant_scale
        activation_bits = layer_info.macro_calc_info.activation_bits
        if layer_info.macro_calc_info.shift_expansion_mode == 'bit_shift':
            input_expansion_method = 1
        elif layer_info.macro_calc_info.shift_expansion_mode == 'bit_pulse':
            input_expansion_method = 0
        else:
            raise ValueError(f'Not support: {layer_info.macro_calc_info.shift_expansion_mode} !!!')
        it_time = layer_info.macro_calc_info.it_time
        wrc = layer_info.macro_mapping_info.row_repeat_num
        # 
        s_ = ''
        if layer_info.macro_mapping_info != None:
            rt = layer_info.macro_mapping_info.runtime
            if rt == 'simulation':
                
                if self.is4search:    
                    s_ += f"x_{layer_name} = Macro_Conv2d_SIM({self.all_node[pre_layers]}, weights['{layer_name}.weight'], kernel={kernel}, stride={stride}, padding={padding}, \n"
                    s_ += f"input_quant_scale={input_quant_scale}, output_dequant_scale={output_dequant_scale}, activation_bits={activation_bits}, \n"
                    s_ += f"input_expansion_method=input_expansion_mode[{self.layer_index[layer_name]}], integration_time=it_time[{self.layer_index[layer_name]}], \n"
                    s_ += f"weight_row_copy=weight_copy_num[{self.layer_index[layer_name]}], PE_weight_noise=PE_weight_noise, device=device)"
                else:
                    s_ += f"x_{layer_name} = Macro_Conv2d_SIM({self.all_node[pre_layers]}, weights['{layer_name}.weight'], kernel={kernel}, stride={stride}, padding={padding}, \n"
                    s_ += f"input_quant_scale={input_quant_scale}, output_dequant_scale={output_dequant_scale}, activation_bits={activation_bits}, \n"
                    s_ += f"input_expansion_method={input_expansion_method}, integration_time={it_time}, "
                    s_ += f"weight_row_copy={wrc}, PE_weight_noise=PE_weight_noise, device=device)"
                    
            elif rt == 'macro':
                
                for k, v in layer_info.macro_mapping_info.mappings.items():
                    chip_id = v.device.split(':')[-1]
                    addr = v.address
                    break
                
                if self.is4search:
                    s_ += f"x_{layer_name}, optim_it_{layer_name} = Macro_Conv2d_Chip_Search({chip_id}, {self.all_node[pre_layers]}, {addr}, weights['{layer_name}.weight'], test_labels['{layer_name}'], kernel={kernel}, stride={stride}, padding={padding}, \n"
                else:
                    s_ += f"x_{layer_name} = Macro_Conv2d_Chip({chip_id}, {self.all_node[pre_layers]}, {addr}, weights['{layer_name}.weight'], kernel={kernel}, stride={stride}, padding={padding}, \n"
                s_ += f"input_quant_scale={input_quant_scale}, output_dequant_scale={output_dequant_scale}, activation_bits={activation_bits}, \n"
                s_ += f"input_expansion_method={input_expansion_method}, integration_time={it_time}, weight_row_copy={wrc}, "
                
                if self.is4program:
                    s_ += f'program=program, )'
                else:
                    s_ += f')'
                
            else:
                raise ValueError(f'Not support device: {rt} !!!')
        else:
            s_ += f"x_{layer_name} = torch.conv2d({self.all_node[pre_layers]}, weights['{layer_name}.weight'], stride={stride}, padding={padding},) \n"    

        return s_
    
    fn_gen_fused_conv2d = fn_gen_conv2d
    
    # conv
    
    def fn_gen_matmul(self, layer_name, layer_info):
        
        # pre layers
        pre_layers = layer_info.inputs[0].ref
        
        input_quant_scale = layer_info.macro_calc_info.input_quant_scale
        output_dequant_scale = layer_info.macro_calc_info.assigned_output_quant_scale
        activation_bits = layer_info.macro_calc_info.activation_bits
        if layer_info.macro_calc_info.shift_expansion_mode == 'bit_shift':
            input_expansion_method = 1
        elif layer_info.macro_calc_info.shift_expansion_mode == 'bit_pulse':
            input_expansion_method = 0
        else:
            raise ValueError(f'Not support: {layer_info.macro_calc_info.shift_expansion_mode} !!!')
        it_time = layer_info.macro_calc_info.it_time
        wrc = layer_info.macro_mapping_info.row_repeat_num
        
        s_ = ''
        if layer_info.macro_mapping_info != None:
            rt = layer_info.macro_mapping_info.runtime
            if rt == 'simulation':
                if self.is4search:    
                    s_ += f"x_{layer_name} = Macro_FC_SIM({self.all_node[pre_layers]}, weights['{layer_name}.weight'], "
                    s_ += f"input_quant_scale={input_quant_scale}, output_dequant_scale={output_dequant_scale}, activation_bits={activation_bits}, \n"
                    s_ += f"input_expansion_method=input_expansion_mode[{self.layer_index[layer_name]}], integration_time=it_time[{self.layer_index[layer_name]}], \n"
                    s_ += f"weight_row_copy=weight_copy_num[{self.layer_index[layer_name]}], PE_weight_noise=PE_weight_noise, device=device)"
                else:
                    s_ += f"x_{layer_name} = Macro_FC_SIM({self.all_node[pre_layers]}, weights['{layer_name}.weight'], "
                    s_ += f"input_quant_scale={input_quant_scale}, output_dequant_scale={output_dequant_scale}, \n activation_bits={activation_bits}, "
                    s_ += f"input_expansion_method={input_expansion_method}, integration_time={it_time}, "
                    s_ += f"weight_row_copy={wrc}, PE_weight_noise=PE_weight_noise, device=device)"
                    
            elif rt == 'macro':
                
                for k, v in layer_info.macro_mapping_info.mappings.items():
                    chip_id = v.device.split(':')[-1]
                    addr = v.address
                    break
                if self.is4search:
                    s_ += f"x_{layer_name}, optim_it_{layer_name} = Macro_FC_Chip_Search({chip_id}, {self.all_node[pre_layers]}, {addr}, weights['{layer_name}.weight'], test_labels['{layer_name}'], \n"
                else:
                    s_ += f"x_{layer_name} = Macro_FC_Chip({chip_id}, {self.all_node[pre_layers]}, {addr}, weights['{layer_name}.weight'], \n"
                s_ += f"input_quant_scale={input_quant_scale}, output_dequant_scale={output_dequant_scale}, activation_bits={activation_bits}, \n"
                s_ += f"input_expansion_method={input_expansion_method}, integration_time={it_time}, weight_row_copy={wrc}, "
                # 
                if self.is4program:
                    s_ += f'program=program, )'
                else:
                    s_ += f')'
            else:
                raise ValueError(f'Not support device: {rt} !!!')
        else:
            s_ += f"x_{layer_name} = F.linear({self.all_node[pre_layers]}, weights['{layer_name}.weight'],) \n"    

        return s_
    
    
    fn_gen_fc = fn_gen_matmul
    fn_gen_fused_fc = fn_gen_matmul
    
    # transes
    
    def fn_gen_concat(self, layer_name, layer_info):
        
        s_ = '['
        c_ = 0
        for in_ in layer_info.inputs:
            s_ += f'{self.all_node[in_.ref]}'
            if c_ != len(layer_info.inputs) - 1:
                s_ += ', '
            c_ += 1
        s_ += ']'
        # attr
        axis = layer_info.op.axis
        return f'x_{layer_name} = Macro_Concat({s_}, axis={axis})'
    
    # activate

    def fn_gen_relu(self, layer_name, layer_info):
        pre_layers = layer_info.inputs[0].ref
        return  f'x_{layer_name} = Macro_Relu({self.all_node[pre_layers]})'
       
        
    def fn_gen_silu(self, layer_name, layer_info):
        # pre layers
        pre_layers = layer_info.inputs[0].ref
        
        return f"x_{layer_name} = Macro_SiLU({self.all_node[pre_layers]})"
    # maxpool2d

    def fn_gen_max_pool2d(self, layer_name, layer_info):
        kernel = layer_info.op.kernel
        stride = layer_info.op.stride
        padding = layer_info.op.padding
        pre_layers = layer_info.inputs[0].ref
        return f'x_{layer_name} = Macro_Maxpool2d({self.all_node[pre_layers]}, kernel_size={kernel}, stride={stride}, padding={padding})'
        
    fn_gen_maxpool2d = fn_gen_max_pool2d

    # avgpool2d

    def fn_gen_avg_pool2d(self, layer_name, layer_info):
        kernel = layer_info.op.kernel
        stride = layer_info.op.stride
        padding = layer_info.op.padding
        pre_layers = layer_info.inputs[0].ref
        return f'x_{layer_name} = Macro_Avgpool2d({self.all_node[pre_layers]}, kernel_size={kernel}, stride={stride}, padding={padding})'
        
    fn_gen_avgpool2d = fn_gen_avg_pool2d
    
    def fn_gen_global_avg_pool2d(self, layer_name, layer_info):
        pre_layers = layer_info.inputs[0].ref
        out_size = layer_info.outputs[0].width
        return f'x_{layer_name} = Macro_GlobalAvgpool2d({self.all_node[pre_layers]}, out_size={out_size})'
        
    # add, mul
    def fn_gen_add(self, layer_name, layer_info):
        
        s_ = '['
        c_ = 0
        for in_ in layer_info.inputs:
            s_ += f'{self.all_node[in_.ref]}'
            if c_ != len(layer_info.inputs) - 1:
                s_ += ', '
            c_ += 1
        s_ += ']'
        
        return f'x_{layer_name} = Macro_Add({s_})'
       
    # resize
    
    def fn_gen_resize(self, layer_name, layer_info):
        
        pre_layer = layer_info.inputs[0].ref 
        scale_factor = layer_info.op.scale[2:]
        return f"x_{layer_name} = Macro_Resize({self.all_node[pre_layer]}, scale_factor={scale_factor})"

    # split
    def fn_gen_split(self, layer_name, layer_info):
        # attr
        dim = layer_info.op.axis
        channel = layer_info.op.split
        pre_layers = layer_info.inputs[0].ref
        # 
        s_ = ''
        c_ = 0
        for o in layer_info.outputs:
            s_ += f'x_{layer_name}_{c_}'
            if c_ != len(layer_info.outputs) - 1:
                s_ += ', '
            c_ += 1
        return f"{s_} = Macro_Split({self.all_node[pre_layers]}, {channel}, dim={dim})"
    
    # identity
    def fn_gen_identity(self, layer_name, layer_info):
        # attr
        pre_layers = layer_info.inputs[0].ref
        return f"x_{layer_name} = {self.all_node[pre_layers]}"
    
    # flatten
    def fn_gen_flatten(self, layer_name, layer_info):
        # attr
        pre_layers = layer_info.inputs[0].ref
        start_dim = layer_info.op.start_dim
        return f"x_{layer_name} = Macro_Flatten({self.all_node[pre_layers]}, start_dim={start_dim})"
    
    # pad
    def fn_gen_pad(self, layer_name, layer_info):
        # attr
        pre_layers = layer_info.inputs[0].ref
        pads = layer_info.op.pads
        # IR attr to torch.Functional attr
        assert len(pads) % 2 == 0, f'pads should be even number, but got {pads} !!!'
        pads_torch = []
        l = len(pads)//2
        for t in range(l-1, -1, -1):
            pads_torch.append(pads[t])
            pads_torch.append(pads[t+l])
        values = layer_info.op.value 
        return f"x_{layer_name} = Macro_Pad({self.all_node[pre_layers]}, pad={pads_torch}, value={values})"  
    
    # batch norm
    def fn_gen_batch_norm2d(self, layer_name, layer_info):
        pre_layers = layer_info.inputs[0].ref
        epsilon = layer_info.op.epsilon
        index_ = layer_name.split('_')[1]
        return f"x_{layer_name} = Macro_BatchNorm2d({self.all_node[pre_layers]},  epsilon={epsilon}, weights=x_BN_{index_}_weights," + \
                f"bias=x_BN_{index_}_bias, mean=x_BN_{index_}_mean, var=x_BN_{index_}_var)"
    
    # softmax
    def fn_gen_softmax(self, layer_name, layer_info):
        # attr
        pre_layers = layer_info.inputs[0].ref
        start_dim = 1
        return f"x_{layer_name} = Macro_Softmax({self.all_node[pre_layers]}, dim={start_dim})"
    
    # tanh
    def fn_gen_tanh(self, layer_name, layer_info):
        # attr
        pre_layers = layer_info.inputs[0].ref
        return f"x_{layer_name} = Macro_Tanh({self.all_node[pre_layers]})"

    # leaky relu
    def fn_gen_leaky_relu(self, layer_name, layer_info):
        # attr
        pre_layers = layer_info.inputs[0].ref
        alpha = layer_info.op.alpha
        return f"x_{layer_name} = Macro_leaky_relu({self.all_node[pre_layers]}, alpha={alpha})"