from ..utils.python import PythonCode
from typing import Callable
from ...irtool.core import BaseIR
from ...irtool.tools import flatten_layers  # noqa
import numpy as np

class TrainingCodeGen(PythonCode):
    
    def __init__(self, ir, module_name='MappedNet', inputs=None, outputs=None, self_define_layer = None, self_define_str = None):
        super().__init__()
        self.ir = ir
        self.module_name = module_name
        self.inputs = inputs
        self.outputs = outputs
        self.self_define_str = self_define_str
        self.self_define_layer = self_define_layer
        
    def gen_notes(self):
        yield '''
# **************************************** #
# This file is automatically generated !!! #
#        Please do not modify it !!!      #
# **************************************** #
'''
    
    def gen_imports(self):
        yield f'import torch'
        yield f'import torch.nn as nn'
        yield f'import torch.nn.functional as F'
        if self.self_define_layer != None:
            yield f'from OpenCIMTC.customized_hat import *'

        
    def gen_layers(self):
        
        yield from self.gen_notes()
        yield from self.gen_imports()
        
        if self.self_define_str != None:
            s = self.self_define_str
            if '\n' in s:
                s = s.split('\n')
                for s_ in s:
                    yield s_
            else:
                yield s
        
        yield 
        # generate function name
        yield f'''class {self.module_name}(nn.Module):'''
        yield 
        # init runtime
        with self.indent():
            if self.self_define_layer != None:
                yield f'def __init__(self, self_define_config, **kwargs):'
            else:
                yield f'def __init__(self):'
            with self.indent():
                yield f'super({self.module_name}, self).__init__()'
        yield
       
        self.ir.layers = dict(self.ir.iter_layers(deep=False, sorted=True)) 
        
        assert isinstance(self.ir, BaseIR), f'invalid IR type={type(self.ir)}'
        layers = self.ir.flatten_layers()
        
        # # init inputs
        # inp, oup = self.ir.get_io_layers(layers)
        
        self.next_layers = self.get_next_layer(layers)
        self.pre_layers = self.get_pre_layer(layers)
        
        remove_layers = []
        if self.inputs != None:
            assert self.inputs in self.pre_layers.keys(), f'{self.inputs}'
            self.get_all_pre_layers(self.inputs, remove_layers)
            remove_layers.append(self.inputs)
            
        if self.outputs != None:
            assert self.outputs in self.next_layers.keys(), f'{self.outputs}'
            self.get_all_next_layers(self.outputs, remove_layers)
        
        if remove_layers != None:
            if self.inputs != None:
                next_layers = self.next_layers[self.inputs]
                for n in next_layers:
                    for j in layers[n].inputs:
                        if j.ref == self.inputs:
                            j.ref = 'graph_input:0'
                layers['graph_input'].inputs = layers[self.inputs].inputs
            if self.outputs != None:
                layers['graph_output'].inputs[0].ref = self.outputs
                layers['graph_output'].inputs[0].width = layers[self.outputs].outputs[0].width
                layers['graph_output'].inputs[0].height = layers[self.outputs].outputs[0].height
                layers['graph_output'].inputs[0].channel = layers[self.outputs].outputs[0].channel
            for n in remove_layers:
                layers.pop(n)
        
        init_str = []

        self.all_node = {}
        self.reuse_layer = {}
        
        for name, layer in layers.items():
            if layer.type == 'input':
                c_ = 0
                for in_ in layer.inputs:
                    self.all_node[f'graph_input:{c_}'] = f'x_graph_input_{c_}'
                    c_ += 1
            elif layer.type == 'op':
                if layer.op.op_id not in ['constant']:
                    if layer.outputs != None and len(layer.outputs) > 1:
                        c_ = 0
                        for in_ in layer.outputs:
                            self.all_node[f'{name}:{c_}'] = f'x_{name}_{c_}'
                            c_ += 1
                    elif layer.op.op_id in ['split']:
                        c_ = 0
                        for in_ in range(layer.op.split):
                            self.all_node[f'{name}:{c_}'] = f'x_{name}_{c_}'
                            c_ += 1
                    else:
                        self.all_node[f'{name}'] = f'x_{name}'
                else:
                    self.all_node[f'{name}'] = f'x_{name}'
            elif layer.type == 'reuse':
                
                self.reuse_layer[name] = layer.layer
                self.all_node[f'{name}'] = f'x_{name}'
                
            if layer.type == 'op' and layer.op.op_id in ['conv2d', 'fused_conv2d', 'matmul', 'fused_fc', 
                                                         'fc', 'maxpool2d', 'batch_norm2d', 'silu', 'resize', 
                                                         'relu', 'avgpool2d', 'global_avg_pool2d', 'conv_transpose2d',
                                                         'layer_norm', 'constant', 'leaky_relu']:
                if layer.op.op_id in ['conv2d', 'fused_conv2d','conv_transpose2d']:
                    y = self.gen_layer(layer.op.op_id, name, layer, gen_code='init')
                else:
                    if layer.op.op_id in ['matmul', 'fused_fc', 'fc',] and len(layer.inputs) == 2:
                        continue
                    y = self.gen_layer(layer.op.op_id, name, layer, gen_code='init')
                    
                init_str.append(y)
    
        # gen init function
        with self.indent():
            with self.indent():
                for s in init_str:
                    print(s)
                    if '\n' in s:
                        s = s.split('\n')
                        for s_ in s:
                            yield s_
                    else:
                        yield s
        
        yield
        
        
        input_layer = layers['graph_input']
        c_ = 0
        with self.indent():
            s_ = ''
            for in_ in input_layer.inputs:
                t = self.all_node[f"graph_input:{c_}"]
                s_ += f'{t}'
                if c_ != len(input_layer.inputs) - 1:
                    s_ += f', '
                c_ += 1
            yield f'def forward(self, {s_}):'
                
        # gen forward function   
        for name, layer in layers.items():
            
            # inner function
            if layer.type in ['op', 'reuse']:
                with self.indent():
                    with self.indent():
                        # 
                        if layer.type == 'reuse':
                            s_ = self.gen_layer(layers[layer.layer].op.op_id, name, layer, gen_code='forward', )
                        else:
                            s_ = self.gen_layer(layer.op.op_id, name, layer, gen_code='forward', )
                        if '\n' in s_:
                            s_1 = s_.split('\n')
                            for s_2 in s_1:
                                yield s_2
                        else:
                            yield s_

            # output function
            if layer.type == 'output':
                with self.indent():
                    with self.indent():
                        s_ = ''
                        c_ = 0
                        for in_ in layer.inputs:
                            if c_ == len(layer.inputs) - 1:
                                s_ += f'{self.all_node[in_.ref]} '
                            else:
                                s_ += f'{self.all_node[in_.ref]}, '
                            c_ += 1
                        yield f'return {s_}'
            
    
    
    def get_pre_layer(self, layers):
        prefix_layer = {}
        for name, layer in layers.items():
            if layer.type not in ['input', 'output']: 
                if layer.type == 'op' and layer.op.op_id in ['constant']:
                    continue
                prefix_layer[name] =  []
                for i in layer.inputs:
                    if 'graph_input' not in i.ref:
                        ref = i.ref
                        if ':' in ref:
                            ref = ref.split(':')[0]
                        prefix_layer[name].append(ref)
                    else:        
                        prefix_layer[name].append(i.ref)
        return prefix_layer
    
    def get_next_layer(self, layers):
        next_layer = {}
        pre_layer = self.get_pre_layer(layers)    
        for k,v in pre_layer.items():
            for name in v:
                if name not in next_layer.keys():
                    next_layer[name] = []
                next_layer[name].append(k)
                
        return next_layer
    
    def get_all_pre_layers(self, layer_name, all_pre_layers):
        if layer_name in self.pre_layers.keys():
            pre_layers = self.pre_layers[layer_name]
            for n in pre_layers:
                if 'graph_input' not in n:
                    if n not in all_pre_layers:
                        all_pre_layers.append(n)
                    self.get_all_pre_layers(n, all_pre_layers)
    
    def get_all_next_layers(self, layer_name, all_next_layers):
        if layer_name in self.next_layers.keys():
            next_layers = self.next_layers[layer_name]
            for n in next_layers:
                if 'graph_output' not in n:
                    if n not in all_next_layers:
                        all_next_layers.append(n)
                    self.get_all_next_layers(n, all_next_layers)
        
    def gen_op(self, op_id, *args, **kwargs):
        fn = getattr(self, f'fn_gen_{op_id}', None)
        assert isinstance(fn, Callable), f'fn_gen_{op_id} is not a function'
        return fn(*args, **kwargs)

    def gen_layer(self, op_id, *args, **kwargs):
        return self.gen_op(op_id, *args, **kwargs)
    
    # matmul
    
    def fn_gen_matmul(self, layer_name, layer_info, gen_code='init'):
        if layer_name in self.reuse_layer.keys():
            layer_info_reuse = self.ir.layers[self.reuse_layer[layer_name]]
            in_channel = layer_info_reuse.op.in_channel
            out_channel = layer_info_reuse.op.out_channel
            bias = layer_info_reuse.op.bias
        else:
            in_channel = layer_info.op.in_channel
            out_channel = layer_info.op.out_channel
            bias = layer_info.op.bias
            
        #   
        if gen_code == 'init':
            train_layer_module = 'nn.Linear'
            if self.self_define_layer != None and 'matmul' in self.self_define_layer:
                train_layer_module = self.self_define_layer['matmul']
                return f"self.{layer_name} = {train_layer_module}({in_channel}, {out_channel}, {str(bias)}, '{layer_name}', self_define_config)"
            else:
                return f'self.{layer_name} = {train_layer_module}({in_channel}, {out_channel}, {str(bias)})'
        
        elif gen_code == 'forward':
            if len(layer_info.inputs) == 1:
                
                pre_layers = layer_info.inputs[0].ref
                if layer_name in self.reuse_layer.keys():
                    s_ = f'x_{layer_name} = self.{self.reuse_layer[layer_name]}({self.all_node[pre_layers]})'
                else:
                    s_ = f'x_{layer_name} = self.{layer_name}({self.all_node[pre_layers]})'

            elif len(layer_info.inputs) == 2:
                pre_layers_1 = layer_info.inputs[0].ref
                pre_layers_2 = layer_info.inputs[1].ref
                s_ = f'x_{layer_name} = torch.matmul({self.all_node[pre_layers_1]}, {self.all_node[pre_layers_2]})'
            return s_
        else:
            raise ValueError(f'Not support: {gen_code} !!!')
        
    fn_gen_linear = fn_gen_matmul
    fn_gen_fc = fn_gen_matmul
    
    # conv
    
    def fn_gen_conv2d(self, layer_name, layer_info, gen_code='init'):
        in_channel = layer_info.op.in_channel
        out_channel = layer_info.op.out_channel
        kernel = layer_info.op.kernel
        stride = layer_info.op.stride
        padding = layer_info.op.padding
        dilation = layer_info.op.dilation
        groups = layer_info.op.groups
        bias = layer_info.op.bias
        if gen_code == 'init':
            train_layer_module = 'nn.Conv2d'
            if self.self_define_layer != None and 'conv2d' in self.self_define_layer:
                train_layer_module = self.self_define_layer['conv2d']
            # 
                return f"self.{layer_name} = {train_layer_module}({in_channel}, {out_channel}, {kernel}, {stride}, {padding}, {dilation}, {groups}, {str(bias)}, '{layer_name}', self_define_config)" 
            else:
                return f'self.{layer_name} = {train_layer_module}({in_channel}, {out_channel}, {kernel}, {stride}, {padding}, {dilation}, {groups}, {str(bias)})' 
        elif gen_code == 'forward':
            pre_layers = layer_info.inputs[0].ref
            s_ = ''
            s_ += f'x_{layer_name} = self.{layer_name}({self.all_node[pre_layers]})'
            return s_
        else:
            raise ValueError(f'Not support: {gen_code} !!!')
        
    # convtranspose2d
    
    def fn_gen_conv_transpose2d(self, layer_name, layer_info, gen_code='init'):
        
        in_channel = layer_info.op.in_channel
        out_channel = layer_info.op.out_channel
        kernel = layer_info.op.kernel
        stride = layer_info.op.stride
        padding = layer_info.op.padding
        bias = layer_info.op.bias
        if gen_code == 'init':
            train_layer_module = 'nn.ConvTranspose2d'
            if 'conv_transpose2d' in self.self_define_layer:
                train_layer_module = self.self_define_layer['conv_transpose2d']
            # 
            s_ = ''
            s_ += f'self.{layer_name} = {train_layer_module}({in_channel}, {out_channel}, {kernel}, {stride}, {padding}, bias={str(bias)})'
            return s_
        elif gen_code == 'forward':
            pre_layers = layer_info.inputs[0].ref
            s_ = ''
            s_ += f'x_{layer_name} = self.{layer_name}({self.all_node[pre_layers]})'
            return s_
        else:
            raise ValueError(f'Not support: {gen_code} !!!')
            
            
    # math
    
    def fn_gen_abs(self, layer_name, layer_info, gen_code='forward'):
        if gen_code == 'forward':
            pre_layer = layer_info.inputs[0].ref
            return f'x_{layer_name} = torch.abs({self.all_node[pre_layer]})'
    
        else:
            raise ValueError(f'Not support {gen_code} !!!')
        
    def fn_gen_acos(self, layer_name, layer_info, gen_code='forward'):
        if gen_code == 'forward':
            pre_layer = layer_info.inputs[0].ref
            return f'x_{layer_name} = torch.acos({self.all_node[pre_layer]})'
    
        else:
            raise ValueError(f'Not support {gen_code} !!!')
    
    # transes
    
    def fn_gen_concat(self, layer_name, layer_info, gen_code='forward'):
        
        if gen_code == 'forward':
            s_ = '['
            c_ = 0
            for in_ in layer_info.inputs:
                s_ += f'{self.all_node[in_.ref]}'
                if c_ != len(layer_info.inputs) - 1:
                    s_ += ', '
                c_ += 1
            s_ += ']'
            dim = layer_info.op.axis
            return f'x_{layer_name} = torch.cat({s_}, dim={dim})'
        
        else:
            raise ValueError(f'Not support {gen_code} !!!')
    
    def fn_gen_constant(self, layer_name, layer_info, gen_code='forward'):
        # value = layer_info.op.value
        if gen_code == 'forward':
            return f'x_{layer_name} = self.x_{layer_name}'
        elif gen_code == 'init':
            value = layer_info.op.value
            value_shape = np.array(value).shape
            if len(value_shape) >= 1:
                return f"self.register_buffer('x_{layer_name}', torch.zeros({value_shape}))"
            else:
                return f"self.register_buffer('x_{layer_name}', torch.tensor({value}))"
        else:
            raise ValueError(f'Not support {gen_code} !!!')

    def fn_gen_flatten(self, layer_name, layer_info, gen_code='forward'):
        if gen_code == 'forward':
            dim = layer_info.op.axis
            pre_layers = layer_info.inputs[0].ref
            return f'x_{layer_name} = torch.flatten({self.all_node[pre_layers]}, start_dim={dim})'
        else:
            raise ValueError(f'Not support {gen_code} !!!')
    
    # activate

    def fn_gen_relu(self, layer_name, layer_info, gen_code='forward'):
        if gen_code == 'forward':
            pre_layers = layer_info.inputs[0].ref
            if layer_name in self.reuse_layer.keys():
                return  f'x_{layer_name} = self.{self.reuse_layer[layer_name]}({self.all_node[pre_layers]})'
            else:
                return  f'x_{layer_name} = self.{layer_name}({self.all_node[pre_layers]})'
        elif gen_code == 'init':
            return f'self.{layer_name} = nn.ReLU()'
        
    def fn_gen_silu(self, layer_name, layer_info, gen_code='forward'):
        if gen_code == 'forward':
            pre_layers = layer_info.inputs[0].ref
            return  f'x_{layer_name} = self.{layer_name}({self.all_node[pre_layers]})'
        elif gen_code == 'init':
            return f'self.{layer_name} = nn.SiLU()'

    # sigmoid
    def fn_gen_sigmoid(self, layer_name, layer_info, gen_code='forward'):
        if gen_code == 'forward':
            pre_layer = layer_info.inputs[0].ref
            return f'x_{layer_name} = torch.sigmoid({self.all_node[pre_layer]})'
        else:
            raise ValueError(f'Not support {gen_code} !!!')
    

    def fn_gen_max_pool2d(self, layer_name, layer_info, gen_code='forward'):
        kernel = layer_info.op.kernel
        stride = layer_info.op.stride
        padding = layer_info.op.padding
        if gen_code == 'init':
            return f'self.{layer_name} = nn.MaxPool2d({kernel}, {stride}, {padding})'
        elif gen_code == 'forward':
            pre_layers = layer_info.inputs[0].ref
            return f'x_{layer_name} = self.{layer_name}({self.all_node[pre_layers]})'
        else:
            raise ValueError(f'Not support {gen_code} !!!')

    fn_gen_maxpool2d = fn_gen_max_pool2d

    def fn_gen_avg_pool2d(self, layer_name, layer_info, gen_code='forward'):
        kernel = layer_info.op.kernel
        stride = layer_info.op.stride
        padding = layer_info.op.padding
        if gen_code == 'init':
            return f'self.{layer_name} = nn.AvgPool2d({kernel}, {stride}, {padding})'
        elif gen_code == 'forward':
            pre_layers = layer_info.inputs[0].ref
            return f'x_{layer_name} = self.{layer_name}({self.all_node[pre_layers]})'
        else:
            raise ValueError(f'Not support {gen_code} !!!')
    
    fn_gen_avgpool2d = fn_gen_avg_pool2d
    
    def fn_gen_global_avg_pool2d(self, layer_name, layer_info, gen_code='forward'):
        output_shape = (layer_info.outputs[0].height,  layer_info.outputs[0].width)
        if gen_code == 'init':
            return f'self.{layer_name} = nn.AdaptiveAvgPool2d({output_shape})'
        elif gen_code == 'forward':
            pre_layers = layer_info.inputs[0].ref
            return f'x_{layer_name} = self.{layer_name}({self.all_node[pre_layers]})'
        else:
            raise ValueError(f'Not support {gen_code} !!!')
    
    # add, mul
    def fn_gen_add(self, layer_name, layer_info, gen_code='forward'):
        
        if gen_code == 'forward':
            full_str = ''
            s_ = ''
            c_ = 0
            for in_ in layer_info.inputs:
                if c_ < 2:
                    s_ += f'{self.all_node[in_.ref]}'
                    if c_ == 0:
                        s_ += ', '
                    if c_ == 1:
                        if len(layer_info.inputs) == 2:
                            full_str += f'x_{layer_name} = torch.add({s_})'
                        else:
                            full_str += f'x_{layer_name}_{c_- 1} = torch.add({s_})\n'
                else:
                    if c_ != len(layer_info.inputs) - 1:
                        full_str += f'x_{layer_name}_{c_- 1} = torch.add(x_{layer_name}_{c_- 2}, {self.all_node[in_.ref]})\n'
                    else:
                        full_str += f'x_{layer_name} = torch.add(x_{layer_name}_{c_- 2}, {self.all_node[in_.ref]})'
                    
                c_ += 1
            # return f'x_{layer_name} = torch.add({s_})'
            return full_str
    
    def fn_gen_mul(self, layer_name, layer_info, gen_code='forward'):
        
        if gen_code == 'forward':
            full_str = ''
            s_ = ''
            c_ = 0
            for in_ in layer_info.inputs:
                if c_ < 2:
                    s_ += f'{self.all_node[in_.ref]}'
                    if c_ == 0:
                        s_ += ', '
                    if c_ == 1:
                        if len(layer_info.inputs) == 2:
                            full_str += f'x_{layer_name} = torch.mul({s_})'
                        else:
                            full_str += f'x_{layer_name}_{c_- 1} = torch.mul({s_})\n'
                else:
                    if c_ != len(layer_info.inputs) - 1:
                        full_str += f'x_{layer_name}_{c_- 1} = torch.mul(x_{layer_name}_{c_- 2}, {self.all_node[in_.ref]})\n'
                    else:
                        full_str += f'x_{layer_name} = torch.mul(x_{layer_name}_{c_- 2}, {self.all_node[in_.ref]})'
                    
                c_ += 1
                
            return full_str
    
    # div
    
    def fn_gen_div(self, layer_name, layer_info, gen_code='forward'):
        if gen_code == 'forward':
            full_str = ''
            s_ = ''
            c_ = 0
            for in_ in layer_info.inputs:
                if c_ < 2:
                    s_ += f'{self.all_node[in_.ref]}'
                    if c_ == 0:
                        s_ += ', '
                    if c_ == 1:
                        if len(layer_info.inputs) == 2:
                            full_str += f'x_{layer_name} = torch.div({s_})'
                        else:
                            full_str += f'x_{layer_name}_{c_- 1} = torch.div({s_})\n'
                else:
                    if c_ != len(layer_info.inputs) - 1:
                        full_str += f'x_{layer_name}_{c_- 1} = torch.div(x_{layer_name}_{c_- 2}, {self.all_node[in_.ref]})\n'
                    else:
                        full_str += f'x_{layer_name} = torch.div(x_{layer_name}_{c_- 2}, {self.all_node[in_.ref]})'
                    
                c_ += 1
                
            return full_str
    
    # resize
    
    def fn_gen_resize(self, layer_name, layer_info, gen_code='forward'):
        if gen_code == 'forward':
            pre_layer = layer_info.inputs[0].ref
            return f'x_{layer_name} = self.{layer_name}({self.all_node[pre_layer]})' 
        elif gen_code == 'init':
            scale = layer_info.op.scale[-1]
            mode = layer_info.op.mode
            return f"self.{layer_name} = nn.Upsample(scale_factor={scale}, mode=f'{mode}', )"
        else:
            raise ValueError(f'Not support {gen_code} !!!')
    
    # reshape
    
    def fn_gen_reshape(self, layer_name, layer_info, gen_code='forward'):
        if gen_code == 'forward':
            pre_layer = layer_info.inputs[0].ref
            shape = layer_info.op.shape
            return f'x_{layer_name} = torch.reshape({self.all_node[pre_layer]}, {shape})' 
        else:
            raise ValueError(f'Not support {gen_code} !!!')
    
    # transpose
    
    def fn_gen_transpose(self, layer_name, layer_info, gen_code='forward'):
        if gen_code == 'forward':
            pre_layer = layer_info.inputs[0].ref
            perm = layer_info.op.perm
            return f'x_{layer_name} = torch.permute({self.all_node[pre_layer]}, {perm})' 
        else:
            raise ValueError(f'Not support {gen_code} !!!')
    
    # tanh
    
    def fn_gen_tanh(self, layer_name, layer_info, gen_code='forward'):
        if gen_code == 'forward':
            pre_layer = layer_info.inputs[0].ref
            return f'x_{layer_name} = torch.tanh({self.all_node[pre_layer]})' 
        else:
            raise ValueError(f'Not support {gen_code} !!!')
    
    # softmax
    
    def fn_gen_softmax(self, layer_name, layer_info, gen_code='forward'):
        if gen_code == 'forward':
            pre_layer = layer_info.inputs[0].ref
            axis = layer_info.op.axis
            return f'x_{layer_name} = F.softmax({self.all_node[pre_layer]}, dim={axis})'    
        else:
            raise ValueError(f'Not support: {gen_code} !!!')
    
    # squeeze
    
    def fn_gen_squeeze(self, layer_name, layer_info, gen_code='forward'):
        if gen_code == 'forward':
            pre_layer = layer_info.inputs[0].ref
            return f'x_{layer_name} = torch.squeeze({self.all_node[pre_layer]})'
        else:
            raise ValueError(f'Not support: {gen_code} !!!')
    
    # unsqueeze
    
    def fn_gen_unsqueeze(self, layer_name, layer_info, gen_code='forward'):
        if gen_code == 'forward':
            pre_layer = layer_info.inputs[0].ref
            axes = layer_info.op.axes[0]
            return f'x_{layer_name} = torch.unsqueeze({self.all_node[pre_layer]}, {axes})'
        else:
            raise ValueError(f'Not support: {gen_code} !!!')    
    
    # split
    
    def fn_gen_split(self, layer_name, layer_info, gen_code='forward'):
        
        if gen_code == 'forward':
            s_ = ''
            c_ = 0
            for o in layer_info.outputs:
                s_ += f'x_{layer_name}_{c_}'
                if c_ != len(layer_info.outputs) - 1:
                    s_ += ', '
                c_ += 1
            dim = layer_info.op.axis
            channel = layer_info.op.split
            pre_layers = layer_info.inputs[0].ref
            return f'{s_} = torch.split({self.all_node[pre_layers]}, {channel}, dim={dim})'
        else:
            raise ValueError(f'Not support: {gen_code} !!!')
    
    # def fn_gen_split(self, layer_name, layer_info, gen_code='forward'):
    
        # if gen_code == 'forward':
        #     s_ = ''
        #     c_ = 0
        #     for o in range(layer_info.op.split):
        #         s_ += f'x_{layer_name}_{c_}'
        #         if c_ != layer_info.op.split - 1:
        #             s_ += ', '
        #         c_ += 1
        #     dim = layer_info.op.axis
        #     if isinstance(layer_info.op.split, int):
        #         pre_layers = self.ir.layers[layer_info.inputs[0].ref]
        #         pre_layers_out_shape = pre_layers.outputs[0].shape[dim]
        #         channel = pre_layers_out_shape // layer_info.op.split
        #     elif isinstance(layer_info.op.split, list):
        #         channel = layer_info.op.split[0]
        #     pre_layers = layer_info.inputs[0].ref
        #     return f'{s_} = torch.split({self.all_node[pre_layers]}, {channel}, dim={dim})'
        # else:
        #     raise ValueError(f'Not support: {gen_code} !!!')
    
    # flatten
    
    def fn_gen_flatten(self, layer_name, layer_info, gen_code='forward'):
        if gen_code == 'forward':
            pre_layer = layer_info.inputs[0].ref
            start_dim = layer_info.op.start_dim
            return f'x_{layer_name} = torch.flatten({self.all_node[pre_layer]}, {start_dim})' 
        else:
            raise ValueError(f'Not support: {gen_code} !!!')
    
    # pad
    
    def fn_gen_pad(self, layer_name, layer_info, gen_code='forward'):
        if gen_code == 'forward':
            pre_layer = layer_info.inputs[0].ref
            pads = layer_info.op.pads
            # convert onnx pad to torch F.pad format
            assert len(pads) % 2 == 0, f' the length of pads should be even, but got {len(pads)} !!!'
            pads_torch = []
            l = len(pads)//2
            for t in range(l-1, -1, -1):
                pads_torch.append(pads[t])
                pads_torch.append(pads[t+l])
            values = layer_info.op.value
            # padding 'constant'
            return f'x_{layer_name} = F.pad({self.all_node[pre_layer]}, {tuple(pads_torch)}, value= {values})' 
        else:
            raise ValueError(f'Not support: {gen_code} !!!')
        
    # batch normalization
    
    def fn_gen_batch_norm2d(self, layer_name, layer_info, gen_code='forward'):
        in_channel = layer_info.op.channel
        if gen_code == 'init':
            s_ = ''
            s_ += f'self.{layer_name} = nn.BatchNorm2d({in_channel}, eps=1e-5)'
            return s_
        elif gen_code == 'forward':
            pre_layers = layer_info.inputs[0].ref
            s_ = ''
            s_ += f'x_{layer_name} = self.{layer_name}({self.all_node[pre_layers]})'
            return s_
        else:
            raise ValueError(f'Not support: {gen_code} !!!')
    
    # layer normalization
    
    def fn_gen_layer_norm(self, layer_name, layer_info, gen_code='forward'):
        axis = layer_info.op.axis
        norm_shape = layer_info.inputs[0].shape[axis]
        eps = layer_info.op.epsilon
        if gen_code == 'init':
            s_ = ''
            s_ += f'self.{layer_name} = nn.LayerNorm({norm_shape}, eps={eps})'
            return s_
        elif gen_code == 'forward':
            pre_layers = layer_info.inputs[0].ref
            s_ = ''
            s_ += f'x_{layer_name} = self.{layer_name}({self.all_node[pre_layers]})'
            return s_
        else:
            raise ValueError(f'Not support: {gen_code} !!!')
    
    
    # erf
    
    def fn_gen_erf(self, layer_name, layer_info, gen_code='forward'):
        if gen_code == 'forward':
            pre_layers = layer_info.inputs[0].ref
            s_ = ''
            s_ += f'x_{layer_name} = torch.erf({self.all_node[pre_layers]})'
            return s_
        else:
            raise ValueError(f'Not support: {gen_code} !!!')
    
    # gather
    
    def fn_gen_gather(self, layer_name, layer_info, gen_code='forward'):
        axis = layer_info.op.axis
        indices = layer_info.op.indices
        if gen_code == 'forward':
            pre_layers = layer_info.inputs[0].ref
            s_ = ''
            if isinstance(indices, int):
                s_ += f'x_{layer_name}_split = torch.split({self.all_node[pre_layers]}, split_size_or_sections=1, dim={axis})[{indices}] \n'
                s_ += f'x_{layer_name} = torch.squeeze(x_{layer_name}_split)'
            else:
                raise ValueError(f'Not support indices type:{type(indices)} !!!')
            return s_
        else:
            raise ValueError(f'Not support: {gen_code} !!!')
    
    # leaky relu
    def fn_gen_leaky_relu(self, layer_name, layer_info, gen_code='forward'):
        alpha = layer_info.op.alpha
        if gen_code == 'init':
            s_ = ''
            s_ += f'self.{layer_name} = nn.LeakyReLU({alpha})'
            return s_
        elif gen_code == 'forward':
            pre_layers = layer_info.inputs[0].ref
            s_ = ''
            s_ += f'x_{layer_name} = self.{layer_name}({self.all_node[pre_layers]})'
            return s_
        else:
            raise ValueError(f'暂不支持 {gen_code} !!!')